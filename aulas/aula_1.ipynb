{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline results are equal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Generate blob data\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=random_seed)\n",
    "\n",
    "# Create pipelines \n",
    "# 1. Using make_pipeline function\n",
    "# 2. Using Pipeline class\n",
    "pipeline_maked = make_pipeline(MinMaxScaler(), StandardScaler())\n",
    "pipeline_with_class = Pipeline([[\"min-max\", MinMaxScaler()], [\"std-scaler\", StandardScaler()]])\n",
    "\n",
    "# Fit and transform the data\n",
    "r_maked = pipeline_maked.fit_transform(X, y)\n",
    "r_with_class = pipeline_with_class.fit_transform(X, y)\n",
    "\n",
    "# Compare, assrting equality\n",
    "assert np.allclose(r_maked, r_with_class)\n",
    "print(\"Pipeline results are equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Minerva model\n",
    "model_str = \"\"\"\n",
    "class_path: minerva.models.nets.classic_ml_pipeline.ClassicMLModel\n",
    "init_args:\n",
    "    backbone:\n",
    "        class_path: torch.nn.Identity\n",
    "        init_args: {}\n",
    "    head: \n",
    "        class_path: minerva.models.nets.classic_ml_pipeline.SklearnPipeline\n",
    "        init_args:\n",
    "            steps:\n",
    "                -   - std-scaler\n",
    "                    - class_path: sklearn.preprocessing.StandardScaler\n",
    "                -   - logreg\n",
    "                    - class_path: sklearn.linear_model.LogisticRegression \n",
    "    use_only_train_data: True\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The DAGHAR Kuhar data module\n",
    "daghar_kuhar_data_module = \"\"\"\n",
    "class_path: minerva.data.data_modules.har.MultiModalHARSeriesDataModule\n",
    "init_args:\n",
    "  data_path: \"/workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar/\"\n",
    "  feature_prefixes: [\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"]\n",
    "  label: \"standard activity code\"\n",
    "  features_as_channels: True\n",
    "  cast_to: \"float32\"\n",
    "  batch_size: 64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Trainer\n",
    "\n",
    "trainer_str = \"\"\"\n",
    "class_path: lightning.Trainer\n",
    "init_args:\n",
    "  # Accelerator to use (CPU, GPU, TPU, ..).\n",
    "  accelerator: cpu\n",
    "  # Number of accelerator devices to use.\n",
    "  devices: 1\n",
    "  # Strategy to use for distributed training.\n",
    "  strategy: auto\n",
    "  # Maximum number of epochs to train.\n",
    "  max_epochs: 1\n",
    "  # Logger and callback configuration (load from file)\n",
    "  deterministic: warn\n",
    "  \n",
    "  \n",
    "  logger: null\n",
    "  callbacks: null\n",
    "  enable_checkpointing: False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file: /tmp/tmp8b4jw7j9/tmpv75zqxao.yaml\n",
      "Data module file: /tmp/tmp8b4jw7j9/tmpcnwxbfrc.yaml\n",
      "Trainer file: /tmp/tmp8b4jw7j9/tmpgbiqstbn.yaml\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "model_file = tempfile.NamedTemporaryFile(dir=tmp_dir.name, suffix=\".yaml\", delete=False)\n",
    "with open(model_file.name, \"w\") as f:\n",
    "    f.write(model_str)\n",
    "    print(f\"Model file: {model_file.name}\")\n",
    "        \n",
    "data_module_file = tempfile.NamedTemporaryFile(dir=tmp_dir.name, suffix=\".yaml\", delete=False)\n",
    "with open(data_module_file.name, \"w\") as f:\n",
    "    f.write(daghar_kuhar_data_module)\n",
    "    print(f\"Data module file: {data_module_file.name}\")\n",
    "    \n",
    "trainer_file = tempfile.NamedTemporaryFile(dir=tmp_dir.name, suffix=\".yaml\", delete=False)\n",
    "with open(trainer_file.name, \"w\") as f:\n",
    "    f.write(trainer_str)  \n",
    "    print(f\"Trainer file: {trainer_file.name}\")\n",
    "   \n",
    "   \n",
    "output_file = tempfile.NamedTemporaryFile(dir=tmp_dir.name, suffix=\".yaml\", delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python -W ignore -m minerva.pipelines.lightning_pipeline --print_config  --model /tmp/tmp8b4jw7j9/tmpv75zqxao.yaml --trainer /tmp/tmp8b4jw7j9/tmpgbiqstbn.yaml run --data /tmp/tmp8b4jw7j9/tmpcnwxbfrc.yaml \n",
      "\n",
      "Output file: /tmp/tmp8b4jw7j9/tmpl8i_5rsx.yaml\n"
     ]
    }
   ],
   "source": [
    "cmd_string = f\"python -W ignore -m minerva.pipelines.lightning_pipeline --print_config  --model {model_file.name} --trainer {trainer_file.name} run --data {data_module_file.name} \"\n",
    "print(f\"Running command: {cmd_string}\\n\")\n",
    "\n",
    "output = !$cmd_string\n",
    "\n",
    "with open(output_file.name, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output))\n",
    "    print(f\"Output file: {output_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  class_path: minerva.models.nets.classic_ml_pipeline.ClassicMLModel\n",
      "  init_args:\n",
      "    backbone:\n",
      "      class_path: torch.nn.Identity\n",
      "    head:\n",
      "      class_path: minerva.models.nets.classic_ml_pipeline.SklearnPipeline\n",
      "      init_args:\n",
      "        steps:\n",
      "        - - std-scaler\n",
      "          - 'Unable to serialize instance {''class_path'': ''sklearn.preprocessing.StandardScaler''}'\n",
      "        - - logreg\n",
      "          - 'Unable to serialize instance {''class_path'': ''sklearn.linear_model.LogisticRegression''}'\n",
      "        memory: null\n",
      "        verbose: false\n",
      "    use_only_train_data: true\n",
      "    test_metrics: null\n",
      "    sklearn_model_save_path: null\n",
      "    flatten: true\n",
      "    adapter: null\n",
      "trainer:\n",
      "  class_path: lightning.Trainer\n",
      "  init_args:\n",
      "    accelerator: cpu\n",
      "    strategy: auto\n",
      "    devices: 1\n",
      "    num_nodes: 1\n",
      "    precision: null\n",
      "    logger: null\n",
      "    callbacks: null\n",
      "    fast_dev_run: false\n",
      "    max_epochs: 1\n",
      "    min_epochs: null\n",
      "    max_steps: -1\n",
      "    min_steps: null\n",
      "    max_time: null\n",
      "    limit_train_batches: null\n",
      "    limit_val_batches: null\n",
      "    limit_test_batches: null\n",
      "    limit_predict_batches: null\n",
      "    overfit_batches: 0.0\n",
      "    val_check_interval: null\n",
      "    check_val_every_n_epoch: 1\n",
      "    num_sanity_val_steps: null\n",
      "    log_every_n_steps: null\n",
      "    enable_checkpointing: false\n",
      "    enable_progress_bar: null\n",
      "    enable_model_summary: null\n",
      "    accumulate_grad_batches: 1\n",
      "    gradient_clip_val: null\n",
      "    gradient_clip_algorithm: null\n",
      "    deterministic: warn\n",
      "    benchmark: null\n",
      "    inference_mode: true\n",
      "    use_distributed_sampler: true\n",
      "    profiler: null\n",
      "    detect_anomaly: false\n",
      "    barebones: false\n",
      "    plugins: null\n",
      "    sync_batchnorm: false\n",
      "    reload_dataloaders_every_n_epochs: 0\n",
      "    default_root_dir: null\n",
      "log_dir: null\n",
      "save_run_status: true\n",
      "classification_metrics: null\n",
      "regression_metrics: null\n",
      "apply_metrics_per_sample: false\n",
      "seed: null\n",
      "run:\n",
      "  data:\n",
      "    class_path: minerva.data.data_modules.har.MultiModalHARSeriesDataModule\n",
      "    init_args:\n",
      "      data_path: /workspaces/HIAAC-KR-Dev-Container/shared_data/daghar/standardized_view/KuHar/\n",
      "      feature_prefixes:\n",
      "      - accel-x\n",
      "      - accel-y\n",
      "      - accel-z\n",
      "      - gyro-x\n",
      "      - gyro-y\n",
      "      - gyro-z\n",
      "      label: standard activity code\n",
      "      features_as_channels: true\n",
      "      transforms: null\n",
      "      cast_to: float32\n",
      "      batch_size: 64\n",
      "      num_workers: null\n",
      "      data_percentage: 1.0\n",
      "  task: null\n",
      "  ckpt_path: null"
     ]
    }
   ],
   "source": [
    "!cat {output_file.name} 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 0\n",
      "Log directory set to: /workspaces/HIAAC-KR-Dev-Container/workspace/lightning_logs/version_1\n",
      "\n",
      "  | Name     | Type     | Params | Mode\n",
      "---------------------------------------------\n",
      "0 | backbone | Identity | 0      | eval\n",
      "---------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "1         Modules in eval mode\n",
      "Epoch 0: 100%|████████████████████████| 22/22 [00:00<00:00, 257.85it/s, v_num=1]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 1/7 [00:00<00:00, 391.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▍             | 2/7 [00:00<00:00, 471.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████████▏          | 3/7 [00:00<00:00, 338.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▊        | 4/7 [00:00<00:00, 383.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|█████████████▌     | 5/7 [00:00<00:00, 417.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████████████▎  | 6/7 [00:00<00:00, 444.16it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████████| 7/7 [00:00<00:00, 465.55it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████████████████████| 22/22 [00:00<00:00, 34.13it/s, v_num=1]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█████████████████████████| 22/22 [00:00<00:00, 34.09it/s, v_num=1]\n",
      "Pipeline info saved at: /workspaces/HIAAC-KR-Dev-Container/workspace/lightning_logs/version_1/run_2024-09-09-18-15-347365634d86cb4de5bc7266be80cb4e92.yaml\n",
      "✨ 🍰 ✨\n"
     ]
    }
   ],
   "source": [
    "! python -m minerva.pipelines.lightning_pipeline --config {output_file.name} run --task fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type     | Params | Mode\n",
      "---------------------------------------------\n",
      "0 | backbone | Identity | 0      | eval\n",
      "---------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 70.21it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 54.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from minerva.models.nets.classic_ml_pipeline import ClassicMLModel, SklearnPipeline\n",
    "from minerva.utils.data import SimpleDataset\n",
    "\n",
    "def test_sklearn_pipeline():\n",
    "    X, y = make_blobs(n_samples=64, centers=2, random_state=42)\n",
    "    train_dataset = SimpleDataset(X, y)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "    pipeline = ClassicMLModel(\n",
    "        backbone = torch.nn.Identity(),\n",
    "        head = SklearnPipeline(\n",
    "            [[\"min-max\", MinMaxScaler()], [\"log-reg\", LogisticRegression(random_state=42, max_iter=5)]]\n",
    "        )\n",
    "    )\n",
    "    trainer = L.Trainer(fast_dev_run=True)\n",
    "    trainer.fit(pipeline, train_dataloader)\n",
    "    \n",
    "test_sklearn_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
